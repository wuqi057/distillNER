{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "#%% ###############third part: loaded trained model and evaluate on testset or external dataset###############\n",
                "path = '/Users/Wu/Google Drive/'\n",
                "\n",
                "from flair.datasets import CONLL_03_GERMAN\n",
                "corpus = CONLL_03_GERMAN(base_path = path ,encoding= 'latin-1' )\n",
                "print('------------------------------')\n",
                "print('training set:',len(corpus.train))\n",
                "print('test set:',len(corpus.test))\n",
                "print('development set',len(corpus.dev))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "2021-08-03 15:16:29,810 Reading data from /Users/Wu/Google Drive/conll_03_german\n",
                        "2021-08-03 15:16:29,810 Train: /Users/Wu/Google Drive/conll_03_german/deu.train\n",
                        "2021-08-03 15:16:29,811 Dev: /Users/Wu/Google Drive/conll_03_german/deu.dev\n",
                        "2021-08-03 15:16:29,811 Test: /Users/Wu/Google Drive/conll_03_german/deu.testb\n",
                        "------------------------------\n",
                        "training set: 12705\n",
                        "test set: 3160\n",
                        "development set 93\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Teachers and students test result on conllu test set"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "%%time\n",
                "from flair.models import SequenceTagger\n",
                "teacher0 = SequenceTagger.load('de-ner')\n",
                "result_t0, eval_loss_t0 = teacher0.evaluate(corpus.test,mini_batch_size=10)\n",
                "print(result_t0.detailed_results) # result for de-ner"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "2021-07-30 00:23:15,336 --------------------------------------------------------------------------------\n",
                        "2021-07-30 00:23:15,337 The model key 'de-ner' now maps to 'https://huggingface.co/flair/ner-german' on the HuggingFace ModelHub\n",
                        "2021-07-30 00:23:15,337  - The most current version of the model is automatically downloaded from there.\n",
                        "2021-07-30 00:23:15,338  - (you can alternatively manually download the original model at https://nlp.informatik.hu-berlin.de/resources/models/de-ner/de-ner-conll03-v0.4.pt)\n",
                        "2021-07-30 00:23:15,339 --------------------------------------------------------------------------------\n",
                        "2021-07-30 00:23:15,941 loading file /Users/Wu/.flair/models/ner-german/a125be40445295f7e94d0afdb742cc9ac40ec4e93259dc30f35220ffad9bf1f6.f46c4c5cfa5e34baa838983373e30051cd1cf1e933499408a49e451e784b0a11\n",
                        "\n",
                        "Results:\n",
                        "- F1-score (micro) 0.7511\n",
                        "- F1-score (macro) 0.6405\n",
                        "\n",
                        "By class:\n",
                        "LOC        tp: 854 - fp: 179 - fn: 181 - precision: 0.8267 - recall: 0.8251 - f1-score: 0.8259\n",
                        "MISC       tp: 49 - fp: 82 - fn: 621 - precision: 0.3740 - recall: 0.0731 - f1-score: 0.1223\n",
                        "ORG        tp: 446 - fp: 101 - fn: 327 - precision: 0.8154 - recall: 0.5770 - f1-score: 0.6758\n",
                        "PER        tp: 1126 - fp: 80 - fn: 69 - precision: 0.9337 - recall: 0.9423 - f1-score: 0.9379\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "source": [
                "%%time\n",
                "from flair.models import SequenceTagger\n",
                "teacher = SequenceTagger.load(\"flair/ner-german-large\")\n",
                "\n",
                "result_t, eval_loss_t = teacher.evaluate(corpus.test,mini_batch_size=10)\n",
                "print(result_t.detailed_results) # result for ner-large"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "2021-07-29 23:06:42,280 loading file /Users/Wu/.flair/models/ner-german-large/6b8de9edd73722050be2547acf64c037b2df833c6e8f0e88934de08385e26c1e.4b0797effcc6ebb1889d5d29784b97f0a099c1569b319d87d7c387e44e2bba48\n",
                        "\n",
                        "Results:\n",
                        "- F1-score (micro) 0.7887\n",
                        "- F1-score (macro) 0.6842\n",
                        "\n",
                        "By class:\n",
                        "LOC        tp: 892 - fp: 151 - fn: 143 - precision: 0.8552 - recall: 0.8618 - f1-score: 0.8585\n",
                        "MISC       tp: 73 - fp: 81 - fn: 597 - precision: 0.4740 - recall: 0.1090 - f1-score: 0.1772\n",
                        "ORG        tp: 496 - fp: 88 - fn: 277 - precision: 0.8493 - recall: 0.6417 - f1-score: 0.7310\n",
                        "PER        tp: 1168 - fp: 45 - fn: 27 - precision: 0.9629 - recall: 0.9774 - f1-score: 0.9701\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "path = '/Users/Wu/Downloads/'\n",
                "from sequence_tagger_model_KD import SequenceTagger\n",
                "student0 = SequenceTagger.load('resources/taggers/ner_25k_30ep/best-model.pt')\n",
                "result_s0, eval_loss_s0 = student0.evaluate(corpus.test,mini_batch_size=10)\n",
                "\n",
                "student1 = SequenceTagger.load('resources/taggers/ner_KD_25k_30ep/best-model.pt')\n",
                "result_s1, eval_loss_s1 = student1.evaluate(corpus.test,mini_batch_size=10)\n",
                "\n",
                "student2 = SequenceTagger.load('resources/taggers/ner_KD_char_25k_30ep/best-model.pt')\n",
                "result_s2, eval_loss_s2 = student2.evaluate(corpus.test,mini_batch_size=10)\n",
                "\n",
                "print(result_s0.detailed_results)\n",
                "print(result_s1.detailed_results)\n",
                "print(result_s2.detailed_results)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "2021-08-03 15:16:42,154 loading file resources/taggers/ner_25k_30ep/best-model.pt\n",
                        "2021-08-03 15:16:53,361 loading file resources/taggers/ner_KD_25k_30ep/best-model.pt\n",
                        "2021-08-03 15:17:05,776 loading file resources/taggers/ner_KD_char_25k_30ep/best-model.pt\n",
                        "\n",
                        "Results:\n",
                        "- F1-score (micro) 0.5281\n",
                        "- F1-score (macro) 0.4413\n",
                        "\n",
                        "By class:\n",
                        "LOC        tp: 495 - fp: 232 - fn: 540 - precision: 0.6809 - recall: 0.4783 - f1-score: 0.5619\n",
                        "MISC       tp: 4 - fp: 66 - fn: 666 - precision: 0.0571 - recall: 0.0060 - f1-score: 0.0108\n",
                        "ORG        tp: 266 - fp: 76 - fn: 507 - precision: 0.7778 - recall: 0.3441 - f1-score: 0.4771\n",
                        "PER        tp: 726 - fp: 109 - fn: 469 - precision: 0.8695 - recall: 0.6075 - f1-score: 0.7153\n",
                        "\n",
                        "Results:\n",
                        "- F1-score (micro) 0.5447\n",
                        "- F1-score (macro) 0.4593\n",
                        "\n",
                        "By class:\n",
                        "LOC        tp: 529 - fp: 244 - fn: 506 - precision: 0.6843 - recall: 0.5111 - f1-score: 0.5852\n",
                        "MISC       tp: 11 - fp: 71 - fn: 659 - precision: 0.1341 - recall: 0.0164 - f1-score: 0.0293\n",
                        "ORG        tp: 286 - fp: 91 - fn: 487 - precision: 0.7586 - recall: 0.3700 - f1-score: 0.4974\n",
                        "PER        tp: 740 - fp: 105 - fn: 455 - precision: 0.8757 - recall: 0.6192 - f1-score: 0.7255\n",
                        "\n",
                        "Results:\n",
                        "- F1-score (micro) 0.6497\n",
                        "- F1-score (macro) 0.5514\n",
                        "\n",
                        "By class:\n",
                        "LOC        tp: 753 - fp: 288 - fn: 282 - precision: 0.7233 - recall: 0.7275 - f1-score: 0.7254\n",
                        "MISC       tp: 24 - fp: 82 - fn: 646 - precision: 0.2264 - recall: 0.0358 - f1-score: 0.0619\n",
                        "ORG        tp: 394 - fp: 142 - fn: 379 - precision: 0.7351 - recall: 0.5097 - f1-score: 0.6020\n",
                        "PER        tp: 885 - fp: 88 - fn: 310 - precision: 0.9096 - recall: 0.7406 - f1-score: 0.8164\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Empirical sentence testing"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "from flair.data import Sentence\n",
                "test_sent = Sentence('In den USA erhalten Nutzerinnen und Nutzer der allerersten Kindle-Modelle derzeit E-Mails vom Hersteller Amazon. Darin werden sie darüber unterrichtet, dass ihre Geräte irgendwann nach dem Dezember 2021 keinen Internetzugang mehr haben werden. Betroffen sind ausschließlich Kindles, die zwischen 2007 und 2009 erworben worden waren – konkret geht es um die Kindle-Modelle der ersten und zweiten Generation sowie den Kindle DX der zweiten Generation. Das berichtet The Verge.')\n",
                "# test_sent = Sentence('Die Mitarbeiter der Internetriesen Google und Facebook in den USA müssen sich vor einer Rückkehr in die Büros gegen das Coronavirus impfen lassen. Das teilten die Unternehmen unabhängig voneinander mit.')\n",
                "\n",
                "teacher0.predict(test_sent)\n",
                "for entity in test_sent.get_spans('ner'):\n",
                "    print(entity)\n",
                "print('\\n')\n",
                "teacher.predict(test_sent)\n",
                "for entity in test_sent.get_spans('ner'):\n",
                "    print(entity)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Span [5]: \"Google\"   [− Labels: ORG (0.9298)]\n",
                        "Span [7]: \"Facebook\"   [− Labels: ORG (0.8869)]\n",
                        "Span [10]: \"USA\"   [− Labels: LOC (1.0)]\n",
                        "Span [21]: \"Coronavirus\"   [− Labels: ORG (0.4415)]\n",
                        "\n",
                        "\n",
                        "Span [5]: \"Google\"   [− Labels: ORG (1.0)]\n",
                        "Span [7]: \"Facebook\"   [− Labels: ORG (1.0)]\n",
                        "Span [10]: \"USA\"   [− Labels: LOC (1.0)]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "student0.predict(test_sent,all_tag_prob = True)\n",
                "for entity in test_sent.get_spans('ner'):\n",
                "    print(entity)\n",
                "print('\\n')\n",
                "\n",
                "student1.predict(test_sent,all_tag_prob = True)\n",
                "for entity in test_sent.get_spans('ner'):\n",
                "    print(entity)\n",
                "print('\\n')\n",
                "\n",
                "student2.predict(test_sent,all_tag_prob = True)\n",
                "for entity in test_sent.get_spans('ner'):\n",
                "    print(entity)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Span [5]: \"Google\"   [− Labels: ORG (0.6898)]\n",
                        "Span [7]: \"Facebook\"   [− Labels: ORG (0.9147)]\n",
                        "Span [10]: \"USA\"   [− Labels: LOC (0.9995)]\n",
                        "\n",
                        "\n",
                        "Span [5]: \"Google\"   [− Labels: ORG (0.9095)]\n",
                        "Span [7]: \"Facebook\"   [− Labels: ORG (0.9555)]\n",
                        "Span [10]: \"USA\"   [− Labels: LOC (0.9996)]\n",
                        "\n",
                        "\n",
                        "Span [5]: \"Google\"   [− Labels: ORG (0.8989)]\n",
                        "Span [7]: \"Facebook\"   [− Labels: ORG (0.951)]\n",
                        "Span [10]: \"USA\"   [− Labels: LOC (0.9989)]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Evaluate Model from Bertram"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "!python3 -m venv venv\n",
                "!source venv/bin/activate\n",
                "!pip3 install numpy"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (1.19.5)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from keras.models import model_from_json\n",
                "path1 = '/Users/Wu/Downloads/resources'\n",
                "name = 'Char_emb_2bilstm_subvoc_e5+3+3leip'\n",
                "\n",
                "model = model_from_json(open(path1 + name + '.json', 'r').read())\n",
                "model.load_weights(path1 + name + '.h5')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "source": [
                "import json\n",
                "path = '/Users/Wu/Google Drive/'\n",
                "sentences = json.load(open(path+\"data/sentences.json\", 'r', encoding='utf-8'))\n",
                "\n",
                "from collections import Counter\n",
                "words = [s.split(' ') for s in sentences]\n",
                "words = [w for s in words for w in s]\n",
                "print(len(words), 'words')\n",
                "words = Counter(words)\n",
                "num_recurring = len([w for w in words if words[w]>1])\n",
                "print(len(words), 'unique words. ', num_recurring, 'words with multiple occurences (>1)')\n",
                "\n",
                "def isfloat(value):\n",
                "  try:\n",
                "    float(value)\n",
                "    return True\n",
                "  except ValueError:\n",
                "    return False\n",
                "    \n",
                "rare_numeric = [w for w in words if words[w]<3 and isfloat(w)]\n",
                "# print('rare numerics:', len(rare_numeric), rare_numeric)\n",
                "for num in rare_numeric:\n",
                "  words.pop(num)\n",
                "\n",
                "word2idx = {w: i + 2 for i, w in enumerate(sorted(words,key = lambda word: words[word],reverse=True))}\n",
                "word2idx[\"UNK\"] = 1\n",
                "word2idx[\"PAD\"] = 0\n",
                "## only words with multiple occurances\n",
                "word2idx = {w:i for w,i in sorted(list(word2idx.items()),key=lambda x:x[1])[:301993]}\n",
                "idx2word = {i: w for w, i in word2idx.items()}"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "16449758 words\n",
                        "1083489 unique words.  410967 words with multiple occurences (>1)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "source": [
                "from flair.models import SequenceTagger\n",
                "path = '/Users/Wu/Google Drive/'\n",
                "tagger = SequenceTagger.load('de-ner')\n",
                "tag2idx = tagger.tag_dictionary.idx2item"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "2021-07-30 15:24:53,755 --------------------------------------------------------------------------------\n",
                        "2021-07-30 15:24:53,757 The model key 'de-ner' now maps to 'https://huggingface.co/flair/ner-german' on the HuggingFace ModelHub\n",
                        "2021-07-30 15:24:53,758  - The most current version of the model is automatically downloaded from there.\n",
                        "2021-07-30 15:24:53,759  - (you can alternatively manually download the original model at https://nlp.informatik.hu-berlin.de/resources/models/de-ner/de-ner-conll03-v0.4.pt)\n",
                        "2021-07-30 15:24:53,760 --------------------------------------------------------------------------------\n",
                        "2021-07-30 15:24:54,356 loading file /Users/Wu/.flair/models/ner-german/a125be40445295f7e94d0afdb742cc9ac40ec4e93259dc30f35220ffad9bf1f6.f46c4c5cfa5e34baa838983373e30051cd1cf1e933499408a49e451e784b0a11\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "source": [
                "path_to_ger = '/Users/Wu/Google Drive/data/NER-de-train.tsv'\n",
                "import numpy as np\n",
                "def get_germeval(path):\n",
                "  f = open(path, 'r', encoding=\"utf-8\")\n",
                "  data = f.read()\n",
                "  data = data.split('\\n\\n')\n",
                "  data = [[line.split('\\t') for line in block.split(\"\\n\")[1:] if line and line[0]!='#'] for block in data]\n",
                "  return np.array(data)\n",
                "\n",
                "data = get_germeval(path_to_ger)\n",
                "print(data[0])\n",
                "tags = set([x[2] for s in data for x in s ])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[['1', 'Schartau', 'B-PER', 'O'], ['2', 'sagte', 'O', 'O'], ['3', 'dem', 'O', 'O'], ['4', '\"', 'O', 'O'], ['5', 'Tagesspiegel', 'B-ORG', 'O'], ['6', '\"', 'O', 'O'], ['7', 'vom', 'O', 'O'], ['8', 'Freitag', 'O', 'O'], ['9', ',', 'O', 'O'], ['10', 'Fischer', 'B-PER', 'O'], ['11', 'sei', 'O', 'O'], ['12', '\"', 'O', 'O'], ['13', 'in', 'O', 'O'], ['14', 'einer', 'O', 'O'], ['15', 'Weise', 'O', 'O'], ['16', 'aufgetreten', 'O', 'O'], ['17', ',', 'O', 'O'], ['18', 'die', 'O', 'O'], ['19', 'alles', 'O', 'O'], ['20', 'andere', 'O', 'O'], ['21', 'als', 'O', 'O'], ['22', 'überzeugend', 'O', 'O'], ['23', 'war', 'O', 'O'], ['24', '\"', 'O', 'O'], ['25', '.', 'O', 'O']]\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "/var/folders/ss/v_ddgqg97bg3d7d6_9s_ty6m0000gp/T/ipykernel_74651/654212356.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
                        "  return np.array(data)\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "source": [
                "data[0]"
            ],
            "outputs": [
                {
                    "output_type": "execute_result",
                    "data": {
                        "text/plain": [
                            "[['1', 'Schartau', 'B-PER', 'O'],\n",
                            " ['2', 'sagte', 'O', 'O'],\n",
                            " ['3', 'dem', 'O', 'O'],\n",
                            " ['4', '\"', 'O', 'O'],\n",
                            " ['5', 'Tagesspiegel', 'B-ORG', 'O'],\n",
                            " ['6', '\"', 'O', 'O'],\n",
                            " ['7', 'vom', 'O', 'O'],\n",
                            " ['8', 'Freitag', 'O', 'O'],\n",
                            " ['9', ',', 'O', 'O'],\n",
                            " ['10', 'Fischer', 'B-PER', 'O'],\n",
                            " ['11', 'sei', 'O', 'O'],\n",
                            " ['12', '\"', 'O', 'O'],\n",
                            " ['13', 'in', 'O', 'O'],\n",
                            " ['14', 'einer', 'O', 'O'],\n",
                            " ['15', 'Weise', 'O', 'O'],\n",
                            " ['16', 'aufgetreten', 'O', 'O'],\n",
                            " ['17', ',', 'O', 'O'],\n",
                            " ['18', 'die', 'O', 'O'],\n",
                            " ['19', 'alles', 'O', 'O'],\n",
                            " ['20', 'andere', 'O', 'O'],\n",
                            " ['21', 'als', 'O', 'O'],\n",
                            " ['22', 'überzeugend', 'O', 'O'],\n",
                            " ['23', 'war', 'O', 'O'],\n",
                            " ['24', '\"', 'O', 'O'],\n",
                            " ['25', '.', 'O', 'O']]"
                        ]
                    },
                    "metadata": {},
                    "execution_count": 37
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "from keras.preprocessing.sequence import pad_sequences\n",
                "from keras.utils import to_categorical\n",
                "from tqdm import tqdm; import numpy as np\n",
                "\n",
                "max_len = 75\n",
                "max_len_char = 17\n",
                "\n",
                "# position of the word token in dataset\n",
                "token_pos = 1\n",
                "\n",
                "X_word_te = [[word2idx.get(w[token_pos], word2idx['UNK']) for w in s] for s in data]\n",
                "X_word_te = pad_sequences(maxlen=max_len, sequences=X_word_te, value=word2idx[\"PAD\"], padding='post', truncating='post')\n",
                "\n",
                "chars = [chr(i) for i in range(32,127)]\n",
                "char2idx = {c: i + 2 for i, c in enumerate(chars)}\n",
                "char2idx[\"UNK\"] = 1\n",
                "char2idx[\"PAD\"] = 0\n",
                "\n",
                "X_char_te = np.zeros((len(data),max_len,max_len_char), dtype=np.int8)\n",
                "for n, sentence in enumerate(tqdm(data)):\n",
                "    for i in range(max_len):\n",
                "        for j in range(max_len_char):\n",
                "            try:\n",
                "                X_char_te[n,i,j] = char2idx.get(sentence[i][token_pos][j],char2idx['UNK'])\n",
                "            except:\n",
                "                X_char_te[n,i,j] = char2idx[\"PAD\"]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "ypred = model.predict([X_word_te,X_char_te], batch_size=128)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "source": [
                "simpletags = {'PAD':0,'O':1,'PER':2,'ORG':3,'LOC':4,'OTH':5,'MIS':5}\n",
                "simpletags_reverse = {v: k for k, v in simpletags.items()}\n",
                "# for t in tags:\n",
                "#   if not t in simpletags:\n",
                "#     simpletags[t] = simpletags[t[2:5]]\n",
                "# for t in tag2idx:\n",
                "#   if t[0] == '<':\n",
                "#     simpletags[t] = 0\n",
                "#   elif not t in simpletags:\n",
                "#     simpletags[t] = simpletags[t[2:5]]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# New Section"
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('3.9')"
        },
        "interpreter": {
            "hash": "b8bdd4e700647ba2b08c59e5df8b7da1dcf50a218bcd4c1bcd9b3dc92e8788e5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}